{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import re\n",
    "\n",
    "\n",
    "#ticker_list = pd.read_csv('ticker_list.txt',header=None)\n",
    "ticker_list = ['RIO', 'XOM', 'GE', 'F', 'MO', 'XRX', 'GS', 'HBC', 'JPM', 'LYG', 'MS', 'RF']\n",
    "headers = {'User-Agent':\n",
    "               'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) '\n",
    "               'AppleWebKit/537.36 (KHTML, like Gecko)'\n",
    "               'Chrome/39.0.2171.95 '\n",
    "               'Safari/537.36'}\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    linklist = []\n",
    "    page = 1\n",
    "    \n",
    "    # crawl all transcripts links for a single ticker \n",
    "    while True:\n",
    "        url = 'https://seekingalpha.com/symbol/'+ ticker + '/earnings/more_transcripts?page=' + str(page)\n",
    "        wb = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(wb.text, 'lxml')\n",
    "        if not soup.select('div div a'):\n",
    "            break\n",
    "        else:\n",
    "            for i in soup.select('div div a'):\n",
    "                linklist.append('https://seekingalpha.com' + i.get('href').strip('\\\"\\\\') + '?part=single')\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(10)\n",
    "    time.sleep(30)\n",
    "    \n",
    "    print('All links got\\n')\n",
    "    \n",
    "    df = pd.DataFrame(columns=['Ticker', 'Date', 'Time'])\n",
    "    df_link = pd.DataFrame(columns=['Link', 'Fail_Link'])\n",
    "    \n",
    "    for ind, link in enumerate(linklist):\n",
    "        print(link)\n",
    "        art = Article(link, language='en', fetch_images=False, memoize_articles=False, browser_user_agent='Mozilla/5.0')\n",
    "        time.sleep(5)\n",
    "        art.download()\n",
    "        \n",
    "        # test whether downloading succeed and store links to link dataframe\n",
    "        try:\n",
    "            art.parse()\n",
    "            df_link.loc[ind, 'Link'] = link\n",
    "        except:\n",
    "            print(link)\n",
    "            df_link.loc[ind, 'Fail_Link'] = link\n",
    "            continue\n",
    "        content = re.split(r'\\n+', art.text)\n",
    "        \n",
    "        try:\n",
    "            file_name = ticker + '_' + content[1].replace(' ', '_') + '.txt'\n",
    "            df.loc[i, 'Ticker'] = ticker\n",
    "            df.loc[i, 'Date'] = content[1]\n",
    "            df.loc[i, 'Time'] = content[2]\n",
    "            \n",
    "            with open(r'/Users/maxwelllee54/PycharmProjects/Project4Yim/output/'+file_name, 'w+') as f:\n",
    "                f.write('\\n'.join(content))\n",
    "            \n",
    "        except IndexError:\n",
    "            print(content)\n",
    "            print(i)\n",
    "            continue \n",
    "        \n",
    "        if not df.empty:\n",
    "            df.to_csv('time_data.csv', mode='a')\n",
    "        \n",
    "        if not df_link.empty:\n",
    "            df.to_csv('link_data.csv', mode='a')\n",
    "        \n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}